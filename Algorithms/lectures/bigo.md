# Big O notation

## Why does it exist?

Often times we need to compare different approaches to solve a problem. Simply put, big o notation is the vocabulary that developers use to talk about how different algorithms perform. When discussing performance, there are a variety of factors that can influence performance that **aren't** the code. Big-O notation is a way to abstract environmental variation/differences out of the way. This allows us to look purely at how the algorithms performance changes as the size of the input varies. it is important to note that there are two primary metrics for algorithm performance: Speed and Memory Consumption.

So, first off, we should talk about abstraction and vocabulary. Big-O notation isn't the only abstraction of performance that exists. There is also big-Theta and big-Omega. What these different notations reflect is the constraints or bounds on abstraction. In this case Omega, Theta, and O correspond to best-case, average-case, and worst-case performance. Typically, best case performance is irrelevant, so theta and O are the most commonly discussed of the three, and even then, O much more than Theta. The abstraction component of big-O comes down to how we label inputs and provide a measure of space or time complexity. Typically, we will look at how the algorithm would handle an infinitely large input. Because of this, we rely on asymptotically defining big-O performance. Asymptotes mean that we look at what shape the run-time approaches as the input gets larger. Asymptotes typically drop smaller constants out of the notation.

Examples:

So now that we have an idea of what big-O is and what it attempts to describe, lets add little bit more vocabulary in to the picture. Really this is just talking about N and M, which are inputs into the algorithm. N typically describes the size of the input (abstracted as it approaches infinity) while M can refer to the dimension of some other input or part of the input that would serve as a form of constant. For example if you have 2 arrays as inputs, the length of one might be N and the other's length ight be M. This translates so that when somebody says an algorithm has a run time of O(N), which translates to 'big O of N', it means that the number of operations performed is directly proportional to the size of the input. Another way to think of big-O notation is describing the number of increased operations per increase in input size. For example if the input is an array and add a single index to its length means that you have to perform an additional operation on every element in the array (think bubble sort) it would be O(N^2).

Cool, so lets talk about the basic/most common performance curves you will see in the wild. Note, big-o notation is its own self contained branch of mathematics, these basic shapes can be combined in a variety of ways to give more complicated descriptions of performance.
